{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HITId', 'HITTypeId', 'Title', 'Description', 'Keywords', 'Reward',\n",
       "       'CreationTime', 'MaxAssignments', 'RequesterAnnotation',\n",
       "       'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds',\n",
       "       'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds',\n",
       "       'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime',\n",
       "       'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime',\n",
       "       'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate',\n",
       "       'Last30DaysApprovalRate', 'Last7DaysApprovalRate', 'Input.Index',\n",
       "       'Input.HS_CITY', 'Input.HS_COUNTRY', 'Input.HS_NAME',\n",
       "       'Input.HS_POSTAL_CD', 'Input.HS_STATE', 'Input.HS_CEEB',\n",
       "       'Input.HS_NCES', 'Input.HS_MATCH_NAME', 'Input.HS_STATE_FULL',\n",
       "       'Answer.HS_NCES', 'Answer.HS_NO_NCES.on', 'Approve', 'Reject'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJ_DIR = path.abspath('.')\n",
    "\n",
    "results = pd.read_csv(path.join(PROJ_DIR,'mturk_results_v3.csv'), dtype=str)\n",
    "golden = pd.read_csv(path.join(PROJ_DIR,'mf_df_golden_resps.csv'), dtype=str)\n",
    "orig_results_columns = results.columns\n",
    "results.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First combine answers into single column for easy processing. Designed \"not in database\" with a string nan (ie. \"NA\") and designated no-answer submissions with a `np.nan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Answer'] = results['Answer.HS_NCES'] \n",
    "results['Answer'][[_=='true' for _ in results['Answer.HS_NO_NCES.on']]] = 'NA'\n",
    "\n",
    "golden['Answer'] = golden['Answer.HS_NCES'] \n",
    "golden['Answer'][[_=='true' for _ in golden['Answer.HS_NO_NCES.on']]] = 'NA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how often each worker \"dissented\" (ie. submitted an answer different from the most common answer) and how often they \"lone dissented\" (ie. were the only worker to submit a different answer)\n",
    "\n",
    "**Note:** I was originally intending to use these as rejection criteria, but ended up not doing so. I'm keeping this in for future reference, and because the code might be handy for determining the \"correct\" answers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "resps = results[['WorkerId','Answer','Input.HS_NAME']].rename(columns={'Input.HS_NAME':'HS_NAME'})\n",
    "\n",
    "worker_dissent_rates = []\n",
    "#resps['lone_dissent'] = \n",
    "for worker in resps.WorkerId.unique():\n",
    "    answers = resps.loc[resps.WorkerId==worker]\n",
    "    \n",
    "    dissents = 0\n",
    "    lone_dissents = 0\n",
    "    total_resps = answers.shape[0]\n",
    "    \n",
    "    for i in range(answers.shape[0]):\n",
    "        if pd.isnull(answers.Answer.iloc[i]):\n",
    "            continue\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        name = answers.HS_NAME.iloc[i]\n",
    "        match_answers = resps.loc[resps.HS_NAME==name]\n",
    "        \n",
    "        modal_answer = answers.Answer.value_counts().idxmax()\n",
    "        \n",
    "        if answers.Answer.iloc[i] != modal_answer:\n",
    "            dissents += 1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if match_answers.Answer.value_counts()[answers.Answer.iloc[i]]==1:\n",
    "            lone_dissents += 1\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    row = {'WorkerId':worker,\n",
    "           'lone_dissent_rate':(lone_dissents/total_resps),\n",
    "           'dissent_rate':(dissents/total_resps),\n",
    "           'total_resps':total_resps\n",
    "          }\n",
    "    worker_dissent_rates.append(row)\n",
    "    \n",
    "worker_dissent_rates = pd.DataFrame(worker_dissent_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate how each worker performed on the \"golden set\" of known answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "known = golden[['WorkerId','Answer','Input.HS_NAME']].rename(columns={'Input.HS_NAME':'HS_NAME'})\n",
    "\n",
    "worker_golden_rates = []\n",
    "for worker in resps.WorkerId.unique():\n",
    "    answers = resps.loc[resps.WorkerId==worker]\n",
    "    answers_known = answers.merge(known,on='HS_NAME')\n",
    "    if answers_known.shape[0]>0:\n",
    "        rate = np.sum(answers_known.Answer_y == answers_known.Answer_x)/answers_known.shape[0]\n",
    "    else:\n",
    "        rate = np.nan\n",
    "    \n",
    "    row = {'WorkerId':worker, 'golden_rate':rate, 'total_golden':answers_known.shape[0]}\n",
    "    worker_golden_rates.append(row)\n",
    "    \n",
    "worker_golden_rates = pd.DataFrame(worker_golden_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine dissent rate and golden set performance into one Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_quality = worker_golden_rates.merge(worker_dissent_rates,on='WorkerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reject all workers who answered 3 or more golden answers and got less than 66% of them correct\n",
    "* **Disregard this criteria:** Reject all workers who were the \"lone dissenter\" more than 75% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 5 workers whose performance was \"low-quality\"\n",
      "There were 1868 responses coming from those workers\n"
     ]
    }
   ],
   "source": [
    "low_golden = (worker_quality.golden_rate<.66) & (worker_quality.total_golden>2)\n",
    "high_lone_dissent = (worker_quality.lone_dissent_rate>.75)\n",
    "# note that low_quality = low_golden\n",
    "low_quality_workers = low_golden\n",
    "\n",
    "low_quality_workers = set(worker_quality.WorkerId.loc[low_quality_workers])\n",
    "print(f'There were {len(low_quality_workers)} workers whose performance was \"low-quality\"')\n",
    "\n",
    "low_quality_resps = [_ in low_quality_workers for _ in resps.WorkerId]\n",
    "print(f'There were {sum(low_quality_resps)} responses coming from those workers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject 1877 answers of 5200 total\n"
     ]
    }
   ],
   "source": [
    "for i in range(results.shape[0]):\n",
    "    if results.WorkerId.iloc[i] in low_quality_workers:\n",
    "        results['Reject'].iloc[i] = 'x'\n",
    "        \n",
    "    elif pd.isnull(resps.Answer.iloc[i]):\n",
    "        results['Reject'].iloc[i] = 'x'\n",
    "    \n",
    "    else:\n",
    "        results['Approve'].iloc[i] = 'x'\n",
    "        \n",
    "n_reject = np.sum(results.Reject=='x')\n",
    "print(f'Reject {n_reject} answers of {results.shape[0]} total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that were added in during grading and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.drop([_ for _ in results.columns if _ not in orig_results_columns], axis=1, inplace=True)\n",
    "results.replace(np.nan,' ',inplace=True)\n",
    "results.to_csv('mturk_results_v3_graded.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966 schools were designated as \"not in the database\" and 286 were matched to a NCES code\n"
     ]
    }
   ],
   "source": [
    "def mode_answer(x):\n",
    "    return(x.value_counts().idxmax())\n",
    "\n",
    "nces = resps.loc[results.Approve=='x'].pivot_table(index='HS_NAME',values='Answer',aggfunc=mode_answer)\n",
    "\n",
    "n_na = np.sum(nces.Answer=='NA')\n",
    "n_nces = nces.shape[0] - np.sum(nces.Answer=='NA')\n",
    "\n",
    "print(f'{n_na} schools were designated as \"not in the database\" and {n_nces} were matched to a NCES code')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condakern",
   "language": "python",
   "name": "condakern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
